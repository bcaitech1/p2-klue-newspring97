{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dba59c38-658b-4017-b1b0-7646f79983be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from load_data import *\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle as pickle\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import json\n",
    "from ipywidgets import FloatProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e793e6b-cb82-42dc-a583-4357a5b57474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace Inference Functions\n",
    "def inference_huggingface(model, tokenized_sent, device):\n",
    "    dataloader = DataLoader(tokenized_sent, batch_size=40, shuffle=False)\n",
    "    model.eval()\n",
    "    output_logits = []\n",
    "\n",
    "    for i, data in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            if 'token_type_ids' in data.keys():\n",
    "                outputs = model(\n",
    "                    input_ids=data['input_ids'].to(device),\n",
    "                    attention_mask=data['attention_mask'].to(device),\n",
    "                    token_type_ids=data['token_type_ids'].to(device)\n",
    "                )\n",
    "            else:\n",
    "                outputs = model(\n",
    "                    input_ids=data['input_ids'].to(device),\n",
    "                    attention_mask=data['attention_mask'].to(device)\n",
    "                )\n",
    "        logits = outputs[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        output_logits.append(logits)\n",
    "    return np.concatenate(output_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b993d2a-8eb5-405c-b2e7-addcd77cb270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_dataset(dataset_dir, tokenizer):\n",
    "    test_dataset = load_data(dataset_dir)\n",
    "    test_label = test_dataset['label'].values\n",
    "    # tokenizing dataset\n",
    "    tokenized_test = tokenized_dataset(test_dataset, tokenizer)\n",
    "    return tokenized_test, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a95716b1-dc10-49b3-9d4f-af6d32b25eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_dir = \"/opt/ml/input/data/test/test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97f98014-c6ab-470c-838d-47e65cf89372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(cfg_file):\n",
    "    print(\"CURR cfg_file: {}\".format(cfg_file))\n",
    "    with open(cfg_file) as f:\n",
    "        cfg = json.load(f)\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    # load tokenizer\n",
    "    TOK_NAME = cfg[\"model_name\"]\n",
    "    #TODO: kobert에 대해 따로 처리할 것\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TOK_NAME)\n",
    "\n",
    "    # load my model\n",
    "    MODEL_NAME = cfg[\"output_dir\"] # model dir.\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        os.path.join(MODEL_NAME, \"checkpoint-{}\".format(cfg[\"num_train_epochs\"] * 550)))\n",
    "    model.parameters\n",
    "    model.to(device)\n",
    "\n",
    "    # load test datset\n",
    "    #test_dataset_dir = \"/opt/ml/input/data/test/test.tsv\"\n",
    "    test_dataset, test_label = load_test_dataset(test_dataset_dir, tokenizer)\n",
    "    test_dataset = RE_Dataset(test_dataset ,test_label)\n",
    "\n",
    "    # predict answer\n",
    "    #print(\"Start prediction...\")\n",
    "    pred_answer = inference_huggingface(model, test_dataset, device)\n",
    "    \n",
    "    return pred_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bd392ce-17aa-4841-a4d2-7097a65b79ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_huggingface = [\n",
    "    'configs/roberta.json',\n",
    "    'configs/bert-seed-7-epoch-20.json',\n",
    "    'configs/koelectra-epoch-20.json',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f0a1320-990c-45b7-8e00-76c0d40a3e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURR cfg_file: configs/roberta.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ml/code/load_data.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.tokenized_dataset.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURR cfg_file: configs/bert-seed-7-epoch-20.json\n",
      "CURR cfg_file: configs/koelectra-epoch-20.json\n",
      "CURR cfg_file: configs/kobert-epoch-20.json\n"
     ]
    }
   ],
   "source": [
    "for path in model_path_huggingface:\n",
    "    result = get_logits(path)\n",
    "    np.save('/opt/ml/logits/logit_{}.npy'.format(path.split('/')[-1].split('.')[0]), result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c662fffe-e032-49e5-8a84-564ed04f45ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.914875  ,  0.3328355 ,  6.9130335 , ..., -1.0136776 ,\n",
       "        -1.067852  , -0.56732583],\n",
       "       [ 0.9859098 , -0.2885439 ,  0.08498568, ..., -0.75621104,\n",
       "        -0.5908805 , -0.7222718 ],\n",
       "       [ 1.0534576 ,  2.995008  , -1.5576702 , ..., -0.58633024,\n",
       "        -0.6264167 , -0.72085637],\n",
       "       ...,\n",
       "       [ 1.0475044 , -0.34612644, -0.02586268, ..., -0.6742935 ,\n",
       "        -0.6050735 , -0.7355977 ],\n",
       "       [ 7.53278   ,  0.17950976,  0.7187565 , ..., -1.7230488 ,\n",
       "        -2.1267328 , -1.5935761 ],\n",
       "       [ 6.6731253 , -0.43922174,  0.38256612, ..., -1.1662935 ,\n",
       "        -1.8269639 , -1.6029122 ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.load('/opt/ml/logits/logit_roberta.npy')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1505b1f4-efac-4bc4-905c-47e04280146c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURR cfg_file: configs/bert-epoch-20.json\n"
     ]
    }
   ],
   "source": [
    "result = get_logits('configs/bert-epoch-20.json')\n",
    "np.save('/opt/ml/logits/logit_bert-epoch-20.npy', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e585966f-4269-4d13-b2e8-74c99a277fde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7024c27-5175-4498-82f7-cbba160524f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 3.0.0\n",
      "Summary: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Sam Shleifer, Patrick von Platen, Google AI Language Team Authors, Open AI team Authors, Facebook AI Authors, Carnegie Mellon University Authors\n",
      "Author-email: thomas@huggingface.co\n",
      "License: Apache\n",
      "Location: /opt/conda/lib/python3.7/site-packages\n",
      "Requires: tokenizers, sentencepiece, numpy, packaging, requests, regex, sacremoses, tqdm, filelock\n",
      "Required-by: kobart\n"
     ]
    }
   ],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e87e32b-259a-4485-b178-fd001b9f779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kobert tokenizer 사용한 logit값 추출\n",
    "from tokenization_kobert import KoBertTokenizer\n",
    "\n",
    "def get_logits_kobert(cfg_file):\n",
    "    print(\"CURR cfg_file: {}\".format(cfg_file))\n",
    "    with open(cfg_file) as f:\n",
    "        cfg = json.load(f)\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    # load tokenizer\n",
    "    TOK_NAME = cfg[\"model_name\"]\n",
    "    #TODO: kobert에 대해 따로 처리할 것\n",
    "    tokenizer = KoBertTokenizer.from_pretrained(TOK_NAME)\n",
    "\n",
    "    # load my model\n",
    "    MODEL_NAME = cfg[\"output_dir\"] # model dir.\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        os.path.join(MODEL_NAME, \"checkpoint-{}\".format(cfg[\"num_train_epochs\"] * 550)))\n",
    "    model.parameters\n",
    "    model.to(device)\n",
    "\n",
    "    # load test datset\n",
    "    #test_dataset_dir = \"/opt/ml/input/data/test/test.tsv\"\n",
    "    test_dataset, test_label = load_test_dataset(test_dataset_dir, tokenizer)\n",
    "    test_dataset = RE_Dataset(test_dataset ,test_label)\n",
    "\n",
    "    # predict answer\n",
    "    #print(\"Start prediction...\")\n",
    "    pred_answer = inference_huggingface(model, test_dataset, device)\n",
    "    \n",
    "    return pred_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fdfb9e3-593c-43bb-a8b1-c4bdd30e1302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.5063766 , -0.47663715, 12.623493  , ..., -0.34894428,\n",
       "        -0.38458663,  1.1262532 ],\n",
       "       [ 1.99646   , -0.42798647,  2.458978  , ..., -1.8876233 ,\n",
       "        -1.6238039 , -1.2348315 ],\n",
       "       [-0.30265176, 10.386776  , -0.8077401 , ..., -2.0220146 ,\n",
       "        -0.3407276 ,  1.3264096 ],\n",
       "       ...,\n",
       "       [ 2.3331583 , -0.44706666,  2.611505  , ..., -2.0147176 ,\n",
       "        -1.6955225 , -1.4178125 ],\n",
       "       [ 5.540759  ,  1.8700576 ,  0.3828238 , ..., -1.0348088 ,\n",
       "        -1.6112745 , -0.81912065],\n",
       "       [ 3.9532156 , -3.3721848 ,  0.86216   , ...,  2.223165  ,\n",
       "        -2.1250982 , -3.461781  ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('/opt/ml/logits/logit_kobert-epoch-20.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f042125-6525-4360-a35d-ffb630e7184f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "paths = glob.glob('/opt/ml/logits/*.npy')\n",
    "results = []\n",
    "\n",
    "for path in paths:\n",
    "    results.append(np.load(path))\n",
    "    \n",
    "np.array(results).sum(axis=0).argmax(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b03506b-5a12-4e3c-a5b5-4097fc4fc201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURR cfg_file: configs/roberta-large-seed-26.json\n"
     ]
    }
   ],
   "source": [
    "cfg_file = 'configs/roberta-large-seed-26.json'\n",
    "print(\"CURR cfg_file: {}\".format(cfg_file))\n",
    "with open(cfg_file) as f:\n",
    "    cfg = json.load(f)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# load tokenizer\n",
    "TOK_NAME = cfg[\"model_name\"]\n",
    "#TODO: kobert에 대해 따로 처리할 것\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOK_NAME)\n",
    "\n",
    "# load my model\n",
    "MODEL_NAME = cfg[\"output_dir\"] # model dir.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    os.path.join(MODEL_NAME, \"checkpoint-2700\"))\n",
    "model.parameters\n",
    "model.to(device)\n",
    "\n",
    "# load test datset\n",
    "#test_dataset_dir = \"/opt/ml/input/data/test/test.tsv\"\n",
    "test_dataset, test_label = load_test_dataset(test_dataset_dir, tokenizer)\n",
    "test_dataset = RE_Dataset(test_dataset ,test_label)\n",
    "\n",
    "# predict answer\n",
    "#print(\"Start prediction...\")\n",
    "pred_answer = inference_huggingface(model, test_dataset, device)\n",
    "\n",
    "np.save('/opt/ml/logits/logit_roberta_large-seed-26.npy', pred_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44fe5775-eb96-4d92-9861-3fff16ce2215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURR cfg_file: configs/bert-with-token.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ml/code/load_data.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.tokenized_dataset.items()}\n"
     ]
    }
   ],
   "source": [
    "cfg_file = 'configs/bert-with-token.json'\n",
    "print(\"CURR cfg_file: {}\".format(cfg_file))\n",
    "with open(cfg_file) as f:\n",
    "    cfg = json.load(f)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# load tokenizer\n",
    "TOK_NAME = cfg[\"model_name\"]\n",
    "#TODO: kobert에 대해 따로 처리할 것\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOK_NAME)\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['[E1], [/E1], [E2], [/E2]']})\n",
    "\n",
    "# load test datset\n",
    "test_dataset_dir = \"/opt/ml/input/data/test/test.tsv\"\n",
    "test_dataset = load_data_with_entity(test_dataset_dir)\n",
    "test_label = test_dataset['label'].values\n",
    "tokenized_test = tokenized_dataset(test_dataset, tokenizer)\n",
    "test_dataset = RE_Dataset(tokenized_test ,test_label)\n",
    "\n",
    "# load my model\n",
    "MODEL_NAME = cfg[\"output_dir\"] # model dir.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    os.path.join(MODEL_NAME, \"checkpoint-11000\"))\n",
    "model.parameters\n",
    "model.to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "# predict answer\n",
    "#print(\"Start prediction...\")\n",
    "pred_answer = inference_huggingface(model, test_dataset, device)\n",
    "\n",
    "np.save('/opt/ml/logits/logit_bert-with-token.npy', pred_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b68f24a8-4d5e-4d8d-b9af-384702d51ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.296289  , -0.04613327,  8.675463  , ..., -1.5508859 ,\n",
       "        -2.844798  , -2.0943873 ],\n",
       "       [ 0.8893885 , -0.32703027,  0.23474614, ..., -1.4912081 ,\n",
       "        -1.1003903 , -0.94410586],\n",
       "       [-1.0814409 ,  2.5289311 , -0.08361945, ..., -0.7561385 ,\n",
       "        -0.14784351, -1.8908087 ],\n",
       "       ...,\n",
       "       [ 1.0392582 , -0.5103955 ,  0.37354428, ..., -1.7872754 ,\n",
       "        -1.2873018 , -0.9500302 ],\n",
       "       [13.0262575 , -0.29220685,  0.28048193, ..., -3.5321977 ,\n",
       "        -4.244457  , -3.6638148 ],\n",
       "       [ 6.444935  , -1.9576925 ,  0.70683753, ..., -0.29889292,\n",
       "        -3.6757355 , -2.274559  ]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98001cc7-2aad-4325-9784-9bf109bd3dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
