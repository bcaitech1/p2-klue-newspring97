{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RmcmqTvs1_T"
   },
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5ZMzORj6Xxn"
   },
   "source": [
    "라이브러리 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oeI3L25s6XZP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet\n",
      "  Downloading mxnet-1.8.0.post0-py2.py3-none-manylinux2014_x86_64.whl (46.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 46.9 MB 8.2 MB/s eta 0:00:01     |█████████████████▍              | 25.4 MB 8.9 MB/s eta 0:00:03     |██████████████████████████▉     | 39.4 MB 7.2 MB/s eta 0:00:02     |████████████████████████████▎   | 41.5 MB 7.2 MB/s eta 0:00:01     |███████████████████████████████▎| 45.9 MB 8.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /opt/conda/lib/python3.7/site-packages (from mxnet) (1.18.5)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /opt/conda/lib/python3.7/site-packages (from mxnet) (2.23.0)\n",
      "Collecting graphviz<0.9.0,>=0.8.1\n",
      "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (2020.6.20)\n",
      "Installing collected packages: graphviz, mxnet\n",
      "Successfully installed graphviz-0.8.4 mxnet-1.8.0.post0\n",
      "Collecting gluonnlp\n",
      "  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n",
      "\u001b[K     |████████████████████████████████| 344 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.1.5)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.46.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from gluonnlp) (1.18.5)\n",
      "Collecting cython\n",
      "  Using cached Cython-0.29.23-cp37-cp37m-manylinux1_x86_64.whl (2.0 MB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from gluonnlp) (20.9)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->gluonnlp) (2.4.7)\n",
      "Building wheels for collected packages: gluonnlp\n",
      "  Building wheel for gluonnlp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595813 sha256=59639c5a38996c7c0ee8b2abe882517951e9a9c1b508c21dd44bb9914b2fe6af\n",
      "  Stored in directory: /opt/ml/.cache/pip/wheels/be/b4/06/7f3fdfaf707e6b5e98b79c041e023acffbe395d78a527eae00\n",
      "Successfully built gluonnlp\n",
      "Installing collected packages: cython, gluonnlp\n",
      "Successfully installed cython-0.29.23 gluonnlp-0.10.0\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.95)\n",
      "Collecting transformers==3\n",
      "  Downloading transformers-3.0.0-py3-none-any.whl (754 kB)\n",
      "\u001b[K     |████████████████████████████████| 754 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==3) (0.0.44)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==3) (4.46.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==3) (3.0.12)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from transformers==3) (0.1.95)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers==3) (1.18.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==3) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==3) (2021.4.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==3) (2.23.0)\n",
      "Collecting tokenizers==0.8.0-rc4\n",
      "  Downloading tokenizers-0.8.0rc4-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 4.2 MB/s eta 0:00:01     |███████▋                        | 727 kB 4.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==3) (2.4.7)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3) (3.0.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3) (1.14.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3) (7.1.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3) (1.0.1)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.10.2\n",
      "    Uninstalling tokenizers-0.10.2:\n",
      "      Successfully uninstalled tokenizers-0.10.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.5.0\n",
      "    Uninstalling transformers-4.5.0:\n",
      "      Successfully uninstalled transformers-4.5.0\n",
      "Successfully installed tokenizers-0.8.0rc4 transformers-3.0.0\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.6.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch) (1.18.5)\n",
      "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
      "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-z2u4wn8i\n",
      "  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-z2u4wn8i\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet\n",
    "!pip install gluonnlp pandas tqdm\n",
    "!pip install sentencepiece\n",
    "!pip install transformers==3\n",
    "!pip install torch\n",
    "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcI3nARqs9qg"
   },
   "source": [
    "라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ETROhbNxsuXQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tarfile\n",
    "import pickle as pickle\n",
    "from tqdm import tqdm\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcobKDe9tAuQ"
   },
   "source": [
    "GPU 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "i8v0khrlswNx"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hZw_ITPtCgp"
   },
   "source": [
    "kobert 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nhsub2pBsx1q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[██████████████████████████████████████████████████]\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9_lv7GMtE1_"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5mr-nvcjOzLF"
   },
   "outputs": [],
   "source": [
    "def load_data(dataset_dir):\n",
    "    with open('/opt/ml/input/data/label_type.pkl', 'rb') as f:\n",
    "        label_type = pickle.load(f)\n",
    "    dataset = pd.read_csv(dataset_dir, delimiter='\\t', header=None)\n",
    "    dataset = preprocessing_dataset(dataset, label_type)\n",
    "    return dataset\n",
    "\n",
    "def preprocessing_dataset(dataset, label_type):\n",
    "    label = []\n",
    "    for i in dataset[8]:\n",
    "        if i == 'blind':\n",
    "            label.append(100)\n",
    "        else:\n",
    "            label.append(label_type[i])\n",
    "    out_dataset = pd.DataFrame({'sentence':dataset[1],'entity_01':dataset[2],'entity_02':dataset[5],'label':label,})\n",
    "    return out_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xkI-E7PauxGq"
   },
   "outputs": [],
   "source": [
    "dataset_path = r\"/opt/ml/input/data/train/train.tsv\"\n",
    "\n",
    "dataset = load_data(dataset_path)\n",
    "\n",
    "dataset['sentence'] = dataset['entity_01'] + ' [SEP] ' + dataset['entity_02'] + ' [SEP] ' + dataset['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YIE_tnYq6AYL"
   },
   "outputs": [],
   "source": [
    "train, vali = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train[['sentence','label']].to_csv(\"/opt/ml/input/data/train/train_train.txt\", sep='\\t', index=False)\n",
    "vali[['sentence','label']].to_csv(\"/opt/ml/input/data/train/train_vali.txt\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2tI-jupiCwpE"
   },
   "outputs": [],
   "source": [
    "dataset_train = nlp.data.TSVDataset(\"/opt/ml/input/data/train/train_train.txt\", field_indices=[0,1], num_discard_samples=1)\n",
    "dataset_vali = nlp.data.TSVDataset(\"/opt/ml/input/data/train/train_vali.txt\", field_indices=[0,1], num_discard_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ca54j41sN-0L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "eRRaHwF_C28c"
   },
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4BKznxZotPrl"
   },
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "batch_size = 32\n",
    "warmup_ratio = 0.01\n",
    "num_epochs = 20\n",
    "max_grad_norm = 1\n",
    "log_interval = 50\n",
    "learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WtW5knVCC6ZC"
   },
   "outputs": [],
   "source": [
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_vali = BERTDataset(dataset_vali, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "spDs0h8tC7fX"
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "vali_dataloader = torch.utils.data.DataLoader(data_vali, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0I1L7EVtShS"
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "eR9IqXuStUbL"
   },
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes = 42,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "piJyyUoutWWt"
   },
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "TqaRnWqwtXii"
   },
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "YYExV_Uwqdpi"
   },
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes=42, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "SvLPsHAMtYp4"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = LabelSmoothingLoss(smoothing=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "wJrYbrK5taVC"
   },
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "4PDk3f8ctasE"
   },
   "outputs": [],
   "source": [
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "7uxhVAqWtcbJ"
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ASK6KHOTtd2H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 1.625742793083191 train acc 0.59375\n",
      "epoch 1 batch id 51 loss 1.461042881011963 train acc 0.6544117647058824\n",
      "epoch 1 batch id 101 loss 1.064346432685852 train acc 0.6760519801980198\n",
      "epoch 1 batch id 151 loss 0.9264274835586548 train acc 0.6910182119205298\n",
      "epoch 1 batch id 201 loss 0.505707323551178 train acc 0.699160447761194\n",
      "epoch 1 train acc 0.7051388888888889\n",
      "epoch 1 test acc 0.6831140350877193\n",
      "epoch 2 batch id 1 loss 1.1351864337921143 train acc 0.6875\n",
      "epoch 2 batch id 51 loss 1.5454633235931396 train acc 0.75\n",
      "epoch 2 batch id 101 loss 0.7211730480194092 train acc 0.7543316831683168\n",
      "epoch 2 batch id 151 loss 0.636289656162262 train acc 0.7613824503311258\n",
      "epoch 2 batch id 201 loss 0.36386698484420776 train acc 0.7705223880597015\n",
      "epoch 2 train acc 0.7743055555555556\n",
      "epoch 2 test acc 0.6913377192982456\n",
      "epoch 3 batch id 1 loss 0.7790242433547974 train acc 0.78125\n",
      "epoch 4 batch id 101 loss 0.5585620403289795 train acc 0.8641707920792079\n",
      "epoch 4 batch id 151 loss 0.4563199281692505 train acc 0.8648592715231788\n",
      "epoch 4 batch id 201 loss 0.12980501353740692 train acc 0.8701803482587065\n",
      "epoch 4 train acc 0.8723611111111111\n",
      "epoch 4 test acc 0.7154605263157895\n",
      "epoch 5 batch id 1 loss 0.760705292224884 train acc 0.78125\n",
      "epoch 5 batch id 51 loss 0.44160157442092896 train acc 0.8927696078431373\n",
      "epoch 5 batch id 101 loss 0.39381277561187744 train acc 0.8923267326732673\n",
      "epoch 5 batch id 151 loss 0.26589953899383545 train acc 0.8903145695364238\n",
      "epoch 5 batch id 201 loss 0.22969985008239746 train acc 0.8910136815920398\n",
      "epoch 5 train acc 0.8930555555555556\n",
      "epoch 5 test acc 0.7044956140350878\n",
      "epoch 6 batch id 1 loss 0.3966608941555023 train acc 0.90625\n",
      "epoch 6 batch id 51 loss 0.4283688962459564 train acc 0.8933823529411765\n",
      "epoch 6 batch id 101 loss 0.3234971761703491 train acc 0.9087252475247525\n",
      "epoch 6 batch id 151 loss 0.30316993594169617 train acc 0.9110099337748344\n",
      "epoch 6 batch id 201 loss 0.44848310947418213 train acc 0.9132462686567164\n",
      "epoch 6 train acc 0.9152777777777777\n",
      "epoch 6 test acc 0.6842105263157895\n",
      "epoch 7 batch id 1 loss 0.34071722626686096 train acc 0.875\n",
      "epoch 7 batch id 51 loss 0.2915143668651581 train acc 0.9185049019607843\n",
      "epoch 7 batch id 101 loss 0.29059749841690063 train acc 0.9251237623762376\n",
      "epoch 7 batch id 151 loss 0.15170863270759583 train acc 0.9250827814569537\n",
      "epoch 7 batch id 201 loss 0.07287879288196564 train acc 0.9269278606965174\n",
      "epoch 7 train acc 0.9286111111111112\n",
      "epoch 7 test acc 0.6792763157894737\n",
      "epoch 8 batch id 1 loss 0.27156898379325867 train acc 0.90625\n",
      "epoch 8 batch id 51 loss 0.4517114758491516 train acc 0.9295343137254902\n",
      "epoch 8 batch id 101 loss 0.26389357447624207 train acc 0.9359529702970297\n",
      "epoch 8 batch id 151 loss 0.16119928658008575 train acc 0.9356374172185431\n",
      "epoch 8 batch id 201 loss 0.1511622667312622 train acc 0.9388992537313433\n",
      "epoch 8 train acc 0.94125\n",
      "epoch 8 test acc 0.6962719298245614\n",
      "epoch 9 batch id 1 loss 0.2603071331977844 train acc 0.875\n",
      "epoch 9 batch id 51 loss 0.18883603811264038 train acc 0.9485294117647058\n",
      "epoch 9 batch id 101 loss 0.2512536942958832 train acc 0.9517326732673267\n",
      "epoch 9 batch id 151 loss 0.1479661613702774 train acc 0.9526076158940397\n",
      "epoch 9 batch id 201 loss 0.04290827736258507 train acc 0.9527363184079602\n",
      "epoch 9 train acc 0.9547222222222222\n",
      "epoch 9 test acc 0.6990131578947368\n",
      "epoch 10 batch id 1 loss 0.12493813782930374 train acc 0.96875\n",
      "epoch 10 batch id 51 loss 0.1360872983932495 train acc 0.9528186274509803\n",
      "epoch 10 batch id 101 loss 0.17274460196495056 train acc 0.9619430693069307\n",
      "epoch 10 batch id 151 loss 0.08900683373212814 train acc 0.9627483443708609\n",
      "epoch 10 batch id 201 loss 0.011731572449207306 train acc 0.9643967661691543\n",
      "epoch 10 train acc 0.9661111111111111\n",
      "epoch 10 test acc 0.7077850877192983\n",
      "epoch 11 batch id 1 loss 0.15405979752540588 train acc 0.96875\n",
      "epoch 11 batch id 51 loss 0.09387712925672531 train acc 0.9662990196078431\n",
      "epoch 11 batch id 101 loss 0.16771502792835236 train acc 0.9693688118811881\n",
      "epoch 11 batch id 151 loss 0.29142501950263977 train acc 0.9712334437086093\n",
      "epoch 11 batch id 201 loss 0.13086582720279694 train acc 0.9712375621890548\n",
      "epoch 11 train acc 0.9722222222222222\n",
      "epoch 11 test acc 0.7088815789473685\n",
      "epoch 12 batch id 1 loss 0.08465467393398285 train acc 0.96875\n",
      "epoch 12 batch id 51 loss 0.028218869119882584 train acc 0.9761029411764706\n",
      "epoch 12 batch id 101 loss 0.16003236174583435 train acc 0.9767945544554455\n",
      "epoch 12 batch id 151 loss 0.050614628940820694 train acc 0.9793046357615894\n",
      "epoch 12 batch id 201 loss 0.009269850328564644 train acc 0.9797885572139303\n",
      "epoch 12 train acc 0.9806944444444444\n",
      "epoch 12 test acc 0.71875\n",
      "epoch 13 batch id 1 loss 0.05748329311609268 train acc 1.0\n",
      "epoch 13 batch id 51 loss 0.05440719425678253 train acc 0.9803921568627451\n",
      "epoch 13 batch id 101 loss 0.14782249927520752 train acc 0.9829826732673267\n",
      "epoch 13 batch id 151 loss 0.047972992062568665 train acc 0.9850993377483444\n",
      "epoch 13 batch id 201 loss 0.008672680705785751 train acc 0.9869402985074627\n",
      "epoch 13 train acc 0.9868055555555556\n",
      "epoch 13 test acc 0.7192982456140351\n",
      "epoch 14 batch id 1 loss 0.049406908452510834 train acc 1.0\n",
      "epoch 14 batch id 51 loss 0.0316217839717865 train acc 0.9859068627450981\n",
      "epoch 14 batch id 101 loss 0.12164280563592911 train acc 0.9879331683168316\n",
      "epoch 14 batch id 151 loss 0.04755701869726181 train acc 0.9888245033112583\n",
      "epoch 14 batch id 201 loss 0.006380074191838503 train acc 0.9895833333333334\n",
      "epoch 14 train acc 0.9898611111111111\n",
      "epoch 14 test acc 0.7214912280701754\n",
      "epoch 15 batch id 1 loss 0.042524270713329315 train acc 1.0\n",
      "epoch 15 batch id 51 loss 0.013274772092700005 train acc 0.9889705882352942\n",
      "epoch 16 batch id 201 loss 0.007107738871127367 train acc 0.9928482587064676\n",
      "epoch 16 train acc 0.9929166666666667\n",
      "epoch 16 test acc 0.7242324561403509\n",
      "epoch 17 batch id 1 loss 0.033264242112636566 train acc 1.0\n",
      "epoch 17 batch id 51 loss 0.019619939848780632 train acc 0.9920343137254902\n",
      "epoch 17 batch id 101 loss 0.10876432061195374 train acc 0.9928836633663366\n",
      "epoch 17 batch id 151 loss 0.03736221045255661 train acc 0.9931705298013245\n",
      "epoch 17 batch id 201 loss 0.00573430210351944 train acc 0.9937810945273632\n",
      "epoch 17 train acc 0.99375\n",
      "epoch 17 test acc 0.7225877192982456\n",
      "epoch 18 batch id 1 loss 0.04243908450007439 train acc 1.0\n",
      "epoch 18 batch id 51 loss 0.019345320761203766 train acc 0.9920343137254902\n",
      "epoch 18 batch id 101 loss 0.10313069075345993 train acc 0.9931930693069307\n",
      "epoch 18 batch id 151 loss 0.02270030602812767 train acc 0.9931705298013245\n",
      "epoch 18 batch id 201 loss 0.00697119627147913 train acc 0.9933146766169154\n",
      "epoch 18 train acc 0.9933333333333333\n",
      "epoch 18 test acc 0.7220394736842105\n",
      "epoch 19 batch id 1 loss 0.03618452325463295 train acc 1.0\n",
      "epoch 19 batch id 51 loss 0.0208860095590353 train acc 0.9944852941176471\n",
      "epoch 19 batch id 101 loss 0.10071614384651184 train acc 0.995049504950495\n",
      "epoch 19 batch id 151 loss 0.029034065082669258 train acc 0.9946192052980133\n",
      "epoch 19 batch id 201 loss 0.0047887833788990974 train acc 0.9948694029850746\n",
      "epoch 19 train acc 0.9944444444444445\n",
      "epoch 19 test acc 0.7231359649122807\n",
      "epoch 20 batch id 1 loss 0.03275541961193085 train acc 1.0\n",
      "epoch 20 batch id 51 loss 0.022378642112016678 train acc 0.9926470588235294\n",
      "epoch 20 batch id 101 loss 0.10181931406259537 train acc 0.9941212871287128\n",
      "epoch 20 batch id 151 loss 0.03374455124139786 train acc 0.9939983443708609\n",
      "epoch 20 batch id 201 loss 0.007252593524754047 train acc 0.9944029850746269\n",
      "epoch 20 train acc 0.9943055555555556\n",
      "epoch 20 test acc 0.725328947368421\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    best_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(vali_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length = valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "    if test_acc >= best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(model.state_dict(), \"/opt/ml/model/model_state_dict.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7ASgrTpfdZh"
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Siuxwi6SdiTW"
   },
   "outputs": [],
   "source": [
    "dataset_path = r\"/opt/ml/input/data/test/test.tsv\"\n",
    "\n",
    "dataset = load_data(dataset_path)\n",
    "\n",
    "dataset['sentence'] = dataset['entity_01'] + ' [SEP] ' + dataset['entity_02'] + ' [SEP] ' + dataset['sentence']\n",
    "\n",
    "dataset[['sentence','label']].to_csv(\"/opt/ml/input/data/test/test.txt\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "yPfoO4ym6AYU"
   },
   "outputs": [],
   "source": [
    "dataset_test = nlp.data.TSVDataset(\"/opt/ml/input/data/test/test.txt\", field_indices=[0,1], num_discard_samples=1)\n",
    "\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "g3TFf_YgtjDG"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"/opt/ml/model/model_state_dict.pt\"))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "Predict = []\n",
    "\n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length = valid_length\n",
    "    label = label.long().to(device)\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "    _, predict = torch.max(out,1)\n",
    "    Predict.extend(predict.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "_aV-Fgpffp4s"
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame(Predict, columns=['pred'])\n",
    "output.to_csv('/opt/ml/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook P2_KLUE.ipynb to script\n",
      "[NbConvertApp] Writing 9079 bytes to P2_KLUE.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script 'P2_KLUE.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "P2-KLUE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
